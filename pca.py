# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yU2QawO_VerZJCFfgk5LEk9tysXiq-o1
"""

# ğŸ“š Importing required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

sns.set(style="whitegrid", palette="coolwarm")
plt.rcParams['figure.figsize'] = (8, 5)

# ğŸ“¥ Load the wine dataset
df = pd.read_csv('/content/wine.csv')

print("âœ… Dataset loaded successfully!")
print("Shape:", df.shape)
df.head()

# ğŸ” Basic Information
df.info()

print("\nğŸ“Š Statistical Summary:")
display(df.describe())

# Check missing values
print("\nMissing Values per Column:\n", df.isnull().sum())

# ğŸ“ˆ Visualizations â€” Feature Distributions and Correlations

# Plot distributions for numeric columns
df.hist(bins=20, figsize=(14,10), color='lightblue')
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()

# Boxplots for outlier detection
for col in df.columns:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=df[col], color='orange')
    plt.title(f"Boxplot for {col}")
    plt.show()

# Correlation heatmap
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap")
plt.show()

# âš™ï¸ Data Standardization before PCA
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

print("âœ… Data successfully standardized.")
print("Mean (approx):", scaled_data.mean().round(2))
print("Std (approx):", scaled_data.std().round(2))

# ğŸ§® Principal Component Analysis (PCA)
pca = PCA()
pca_data = pca.fit_transform(scaled_data)

# Explained variance ratio
explained_variance = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(7,5))
plt.plot(range(1, len(explained_variance)+1), explained_variance, marker='o', color='red')
plt.title("Cumulative Explained Variance by PCA Components")
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.grid(True)
plt.show()

# Optimal components (e.g., 2 for 90-95% variance)
n_components = np.argmax(explained_variance >= 0.95) + 1
print(f"âœ… Optimal number of components to retain 95% variance: {n_components}")

# ğŸ”„ Transform dataset into PCA space
pca_final = PCA(n_components=n_components)
pca_transformed = pca_final.fit_transform(scaled_data)

print("âœ… PCA Transformation Completed!")
print("Shape after PCA:", pca_transformed.shape)

# ğŸ§  K-MEANS CLUSTERING â€” ORIGINAL DATA

# Find optimal number of clusters using Elbow method
inertia = []
K = range(2, 11)
for k in K:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(scaled_data)
    inertia.append(km.inertia_)

plt.figure(figsize=(7,4))
plt.plot(K, inertia, 'bo--')
plt.title("Elbow Method (Original Data)")
plt.xlabel("K")
plt.ylabel("Inertia")
plt.show()

# Apply K-Means (choose optimal K, e.g., 3)
kmeans_orig = KMeans(n_clusters=3, random_state=42)
orig_labels = kmeans_orig.fit_predict(scaled_data)

# Evaluation
sil_orig = silhouette_score(scaled_data, orig_labels)
db_orig = davies_bouldin_score(scaled_data, orig_labels)

print(f"ğŸ“Š Original Data Clustering Metrics:")
print(f"Silhouette Score: {sil_orig:.3f}")
print(f"Daviesâ€“Bouldin Index: {db_orig:.3f}")

# Visualization of first two features
plt.figure(figsize=(7,5))
sns.scatterplot(x=scaled_data[:,0], y=scaled_data[:,1], hue=orig_labels, palette="tab10")
plt.title("K-Means Clustering on Original Data")
plt.show()

# ğŸ§  K-MEANS CLUSTERING â€” PCA-REDUCED DATA
inertia_pca = []
for k in K:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(pca_transformed)
    inertia_pca.append(km.inertia_)

plt.figure(figsize=(7,4))
plt.plot(K, inertia_pca, 'go--')
plt.title("Elbow Method (PCA Data)")
plt.xlabel("K")
plt.ylabel("Inertia")
plt.show()

# Apply K-Means (same K = 3)
kmeans_pca = KMeans(n_clusters=3, random_state=42)
pca_labels = kmeans_pca.fit_predict(pca_transformed)

# Evaluation
sil_pca = silhouette_score(pca_transformed, pca_labels)
db_pca = davies_bouldin_score(pca_transformed, pca_labels)

print(f"ğŸ“Š PCA Data Clustering Metrics:")
print(f"Silhouette Score: {sil_pca:.3f}")
print(f"Daviesâ€“Bouldin Index: {db_pca:.3f}")

# Visualization of PCA components
plt.figure(figsize=(7,5))
sns.scatterplot(x=pca_transformed[:,0], y=pca_transformed[:,1], hue=pca_labels, palette="viridis")
plt.title("K-Means Clustering on PCA Data")
plt.show()

# âš–ï¸ COMPARISON: ORIGINAL vs PCA CLUSTERING

comparison = pd.DataFrame({
    'Metric': ['Silhouette Score', 'Daviesâ€“Bouldin Index'],
    'Original Data': [sil_orig, db_orig],
    'PCA Data': [sil_pca, db_pca]
})
display(comparison)

print("""
ğŸ’¡ OBSERVATIONS:
- PCA-reduced data often provides cleaner, faster clustering.
- Dimensionality reduction helps eliminate noise and redundancy.
- Slight variation in silhouette and DB index reflects structure simplification.
- PCA visualization makes clusters easier to interpret.
""")

# ğŸ CONCLUSION & INSIGHTS
print("""
âœ” PCA successfully reduced dataset dimensions while preserving ~95% variance.
âœ” Clustering on PCA data produced comparable or better silhouette scores.
âœ” PCA helps improve performance and visualization in high-dimensional datasets.
âœ” Trade-off: Minor loss in detail vs significant gain in computational efficiency.

âœ… Recommendation:
Use PCA before clustering when:
- The dataset has many correlated variables.
- Speed and interpretability are key.
- Slight accuracy trade-offs are acceptable.
""")

